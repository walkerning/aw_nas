# param: 14.7 M
# flops: 626 M
# clean acc: 0.932

## ---- Component search_space ----
# ---- Type cnn ----
search_space_type: cnn
search_space_cfg:
  # Schedulable attributes: 
  cell_layout: [0]
  num_cell_groups: 1
  num_init_nodes: 1
  num_layers: 1
  num_node_inputs: 1
  num_steps: 1
  reduce_cell_groups:
  - 1
  shared_primitives:
  - none
# ---- End Type cnn ----
## ---- End Component search_space ----

## ---- Component dataset ----
# ---- Type cifar10 ----
dataset_type: cifar10
dataset_cfg:
  # Schedulable attributes: 
  cutout: null
# ---- End Type cifar10 ----
## ---- End Component dataset ----

## ---- Component final_model ----
# ---- Type cnn_genotype ----
final_model_type: cnn_final_model
final_model_cfg:
  # Schedulable attributes: dropout_path_rate
  genotypes: "normal_0=[('nin', 0, 1)]"
  auxiliary_cfg: null
  auxiliary_head: false
  # layer_channels: [64, 64, 128, 128, 256, 256, 256, 512, 512, 512, 512, 512, 512]
  layer_channels: [3,10]
  dropout_path_rate: 0.0
  dropout_rate: 0.0
  init_channels: 3
  num_classes: 10
  cell_use_preprocess: false
  schedule_cfg: null
  stem_multiplier: 1
  use_stem: false
  no_fc: True
  stem_stride: 1
  stem_affine: true
# ---- End Type cnn_genotype ----
## ---- End Component final_model ----

objective_type: classification
objective_cfg: {}

## ---- Component final_trainer ----
# ---- Type cnn_trainer ----
final_trainer_type: cnn_trainer
final_trainer_cfg:
  # Schedulable attributes: 
  auxiliary_head: false
  auxiliary_weight: 0.0
  add_regularization: true
  batch_size: 256
  epochs: 200
  # grad_clip: 5.0
  learning_rate: 0.0005
  momentum: 0.9
  no_bias_decay: false
  optimizer_type: Adam
  optimizer_scheduler:
    eta_min: 0.00
    T_max: 300
    type: CosineAnnealingLR
    # type: MultiStepLR
    # milestones: [120, 200, 240, 280] 
    #  gamma: 0.1
  schedule_cfg: null
  warmup_epochs: 0
  weight_decay: 0.0
  save_as_state_dict: true
# ---- End Type cnn_trainer ----
## ---- End Component final_trainer ----
